**Incident handler's journal**

| Date:  15/07/2025 | Entry: 1 |  |  |
| :---- | :---- | ----- | ----- |
| Description | Multiple reports were received from employees about the inability to access files and the appearance of ransom notes on their computers. A widespread disruption of operations has been confirmed. |  |  |
| Tool(s) used | N/A |  |  |
| The 5 W's  | **Who** caused the incident? a well-known group of unethical hackers specializing in ransomware attacks against the healthcare and transportation sectors **What** happened? Ransomware attack that encrypted critical company files, including patient medical records, rendering computer systems and programs inaccessible. A large sum of money is being demanded for the decryption key. **When** did the incident occur? Tuesday, July 15, 2025, at approximately 9:00 a.m. (local clinic time). **Where** did the incident happen? At a small US healthcare clinic specializing in primary care, affecting employee computers and the network. **Why** did the incident happen? The attackers gained initial access to the network through targeted phishing emails with a malicious attachment, which installed malware and then deployed the ransomware. |  |  |
| Additional notes | The company has been forced to shut down its systems. The relevant authorities and an external incident response team must be contacted urgently. Significant impact on the availability of critical patient data and operations. |  |  |

---

| Date:  16/07/2025 | Entry: 2 |  |  |
| :---- | :---- | ----- | ----- |
| Description | Provide a An alert from the SIEM detected unusual outbound data transfer volumes from a marketing department server. Further investigation revealed a compromised user account attempting to access sensitive customer database backups. This activity was highly suspicious given the user's normal behavior.description about the journal entry. |  |  |
| Tool(s) used | SIEM (Splunk): Used for initial alert detection, log aggregation, and correlation of network traffic and authentication logs. Active Directory Logs: Reviewed to confirm login times and locations for the compromised user account. Network Packet Analyzer (Wireshark/tcpdump): Utilized to inspect the suspicious outbound traffic and identify the destination of the exfiltrated data. |  |  |
| The 5 W's  |  **Who** caused the incident? An external threat actor, likely exploiting previously leaked credentials of an employee in the marketing department. **What** happened? Unauthorized access to a marketing server and attempted exfiltration of a customer database backup. Data integrity and confidentiality are at risk. **When** did the incident occur? The anomalous activity was detected on Wednesday, July 16, 2025, between 02:00 AM and 04:30 AM local time. **Where** did the incident happen? On a file server in the marketing department's network segment and affecting a cloud storage service used for backups. **Why** did the incident happen? A user's credentials (username and password) were likely compromised through a credential stuffing attack or a prior data breach, allowing the attacker to authenticate as a legitimate user. |  |  |
| Additional notes | This incident highlights the critical need for **Multi-Factor Authentication (MFA)** across all user accounts, especially those with access to sensitive data. The SIEM's ability to correlate seemingly disparate logs (network traffic \+ authentication) was crucial for early detection. We must implement stricter access controls and continuous monitoring of outbound data flows. Post-incident, a forensic analysis of the compromised workstation and server is imperative to identify the initial point of compromise and ensure no other backdoors exist. User awareness training on credential hygiene is also a priority. |  |  |

---

| Date:  17/07/2025 | Entry: 3 |  |  |
| :---- | :---- | ----- | ----- |
| Description | Our main e-commerce website experienced a sudden and severe outage, becoming completely inaccessible to customers. Monitoring tools indicated a massive surge in traffic, far exceeding normal levels. Customer complaints started flooding in, impacting sales and reputation significantly. |  |  |
| Tool(s) used | Network Monitoring Tools (Grafana/Zabbix): Provided real-time visibility into traffic volume, latency, and server load, confirming the attack's nature. Firewall/WAF Logs: Analyzed to identify the source IP addresses and patterns of the malicious traffic. Cloud Provider's DDoS Mitigation Services: Engaged immediately to filter and absorb the malicious traffic. |  |  |
| The 5 W's  | **Who** caused the incident? An unidentified malicious actor or botnet, likely targeting the company for financial disruption or reputational damage. **What** happened? A Distributed Denial of Service (DDoS) attack overwhelmed the e-commerce website's servers, rendering it unavailable to legitimate users. **When** did the incident occur? The attack began abruptly on Thursday, July 17, 2025, at approximately 11:00 AM (local time), lasting for about 3 hours. **Where** did the incident happen? Targeting the external-facing web servers and load balancers of the company's e-commerce platform. **Why** did the incident happen?  The motivation is currently unknown, but could be related to extortion, competitive sabotage, or ideological reasons. The attackers likely utilized a large botnet to generate the traffic volume. |  |  |
| Additional notes | The rapid response capabilities of our **cloud-based DDoS mitigation service** proved invaluable in reducing the attack's duration and impact. This incident underscores the importance of having robust **DDoS protection strategies** in place, including rate limiting and geographical filtering. We need to review our **CDN (Content Delivery Network)** configuration for optimal performance during such events. Future efforts should focus on analyzing traffic patterns captured during the attack to identify common signatures or sources, potentially leading to pre-emptive blocking. Also, post-incident communication with affected customers must be transparent to maintain trust. |  |  |

---

| Date:  18/07/2025 | Entry: 4 |  |  |
| :---- | :---- | ----- | ----- |
| Description | A security scanner identified a critical vulnerability on an internal web application server. Soon after, unusual processes were observed running on that server, indicating potential unauthorized access and a successful exploit. This could lead to a broader network compromise if not addressed swiftly. |  |  |
| Tool(s) used | Vulnerability Scanner (Nessus/OpenVAS): Used for proactive detection of the software vulnerability on the web server. Endpoint Detection and Response (EDR) Tool: Alerted on the anomalous process execution and provided forensic capabilities on the compromised server. Log Management System (Elastic Stack): Centralized system logs were analyzed to trace the attacker's initial actions and determine persistence mechanisms. |  |  |
| The 5 W's  | **Who** caused the incident? An opportunistic threat actor exploiting a known, but unpatched, vulnerability in the web application software. **What** happened? A critical web application server was compromised due to a software vulnerability, allowing unauthorized process execution and potential remote code execution. **When** did the incident occur? The vulnerability was identified by the scanner on Friday, July 18, 2025, in the morning. Anomalous processes began appearing around 11:45 AM (local time). **Where** did the incident happen? On a web application server within the internal network segment. **Why** did the incident happen? The server was running an outdated version of a web application with a publicly known and exploitable vulnerability (CVE). Patch management procedures were not followed rigorously, creating an open window for attack. |  |  |
| Additional notes | This incident is a stark reminder of the paramount importance of a rigorous **patch management program** and **vulnerability scanning schedule**. We must immediately apply the necessary patches and review the entire patching pipeline. The EDR tool's ability to detect anomalous processes was key to catching this quickly. We need to implement more stringent **segmentation** for web application servers to limit lateral movement in case of compromise. A thorough **root cause analysis** is needed to understand why the patch was not applied in time. Prioritizing software updates based on vulnerability severity and exposure is non-negotiable for reducing our attack surface. |  |  |

---

| Date:  19/07/2025 | Entry: 5 |  |  |
| :---- | :---- | ----- | ----- |
| Description |  A finance department employee reported receiving a suspicious email seemingly from the CEO, requesting an urgent wire transfer to an unknown vendor. The employee, noticing slight inconsistencies, escalated the email to the security team before processing the request. Investigation revealed it was a sophisticated phishing attempt targeting Business Email Compromise (BEC). |  |  |
| Tool(s) used | Email Gateway Security (Mimecast/Proofpoint): Reviewed logs to track the origin and headers of the suspicious email, identifying its malicious nature and external source. Endpoint Detection and Response (EDR) Tool: Used to scan the employee's workstation for any potential malware or persistent access, though none was found due to early detection. Security Awareness Platform: Provided training materials on recognizing phishing attempts, which likely contributed to the employee's vigilance.  |  |  |
| The 5 W's  | **Who** caused the incident? An organized cybercriminal group specializing in BEC scams, likely after extensive reconnaissance of our company's executive communications. **What** happened? A highly targeted phishing email attempted to trick a finance employee into making an unauthorized wire transfer, posing as the CEO. **When** did the incident occur? The phishing email was received on Friday, July 19, 2025, at approximately 10:15 AM (local time). **Where** did the incident happen? The attack originated externally, targeting an employee's email inbox within the finance department. **Why** did the incident happen? The attackers aimed to defraud the company by exploiting trust and manipulating employees to initiate fraudulent financial transactions. They likely researched public information or previous email compromises to craft a believable lure. |  |  |
| Additional notes | This incident underscores the critical importance of **employee security awareness training**, particularly for departments handling financial transactions. The employee's quick thinking prevented significant financial loss. We need to implement stronger **email authentication protocols (SPF, DKIM, DMARC)** to prevent email spoofing. Additionally, reviewing and enforcing **strict financial transfer policies** (e.g., verbal confirmation for large sums) is crucial. We should also consider deploying **AI-driven email security solutions** that can detect subtle anomalies in email communication patterns. This incident serves as a valuable case study for future training sessions. |  |  |

---

| Date:  20/07/2025 | Entry: 6 |  |  |
| :---- | :---- | ----- | ----- |
| Description |  Automated alerts from our SIEM indicated a massive volume of failed SSH login attempts on an internet-facing development server. The attempts originated from a wide range of global IP addresses, suggesting a distributed brute-force attack targeting common usernames. The server's performance was degraded due to the high load. |  |  |
| Tool(s) used | SIEM (Elastic Stack): Identified the high volume of failed login attempts, correlating them across multiple source IPs and detecting the brute-force pattern. Firewall: Used to implement temporary IP blocking rules (rate limiting) to mitigate the ongoing attack and reduce server load. Fail2Ban (or similar Host-based Intrusion Prevention): Reviewed its logs to confirm automated blocking of persistent attacking IPs. SSH Server Logs: Directly examined to verify specific usernames targeted and the sheer volume of attempts. |  |  |
| The 5 W's  | **Who** caused the incident? Unknown automated botnet, likely conducting wide-scale scanning for vulnerable SSH services to gain unauthorized access. **What** happened? A brute-force attack attempted to guess passwords for SSH access on a development server by trying numerous username/password combinations. **When** did the incident occur? The attack started on Saturday, July 20, 2025, around 03:00 AM (local time) and peaked over several hours. **Where** did the incident happen? Targeting the SSH service of a specific development server exposed to the internet. **Why** did the incident happen? The attackers aimed to gain unauthorized remote access to the server. The server was likely discovered through automated internet scans for open SSH ports. |  |  |
| Additional notes | This incident highlights the continuous threat of **automated attacks** against internet-facing services. While `Fail2Ban` and firewall rules helped, we need to enforce stricter **SSH key-based authentication** instead of passwords for all external-facing servers. Implementing a **VPN for all administrative access** to internal resources would further reduce exposure. Regularly reviewing **SSH daemon configurations** (e.g., disallowing root login, changing default ports, increasing complexity of brute-force prevention) is essential. Also, monitoring **outbound connections** from development servers is critical, as compromise could lead to lateral movement. |  |  |

